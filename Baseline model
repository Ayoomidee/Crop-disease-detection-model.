import numpy as np
import os
import cv2
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report
from sklearn.neighbors import KNeighborsClassifier

class DeepNeuralNetScratch:
    """
    An improved Deep Neural Network implementation from scratch using NumPy.
    Includes:
    - 2 Layers (Hidden + Output)
    - ReLU Activation
    - Mini-batch Gradient Descent
    - Manual Backpropagation
    """
    def __init__(self, input_dim, hidden_dim, num_classes, learning_rate=0.01):
        # Layer 1 (Hidden)
        self.w1 = np.random.randn(input_dim * input_dim, hidden_dim) * np.sqrt(2. / (input_dim * input_dim))
        self.b1 = np.zeros((1, hidden_dim))
        
        # Layer 2 (Output)
        self.w2 = np.random.randn(hidden_dim, num_classes) * np.sqrt(2. / hidden_dim)
        self.b2 = np.zeros((1, num_classes))
        
        self.lr = learning_rate

    def relu(self, x):
        return np.maximum(0, x)

    def relu_derivative(self, x):
        return (x > 0).astype(float)

    def softmax(self, x):
        exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))
        return exp_x / np.sum(exp_x, axis=1, keepdims=True)

    def forward(self, X):
        # Flatten input
        self.a0 = X.reshape(X.shape[0], -1)
        
        # Hidden Layer
        self.z1 = np.dot(self.a0, self.w1) + self.b1
        self.a1 = self.relu(self.z1)
        
        # Output Layer
        self.z2 = np.dot(self.a1, self.w2) + self.b2
        self.probs = self.softmax(self.z2)
        return self.probs

    def train_step(self, X_batch, y_batch):
        batch_size = X_batch.shape[0]
        
        # 1. Forward Pass
        probs = self.forward(X_batch)
        
        # 2. Backward Pass (Backpropagation)
        # Gradient of Output Layer
        y_one_hot = np.zeros_like(probs)
        y_one_hot[np.arange(batch_size), y_batch] = 1
        dz2 = (probs - y_one_hot) / batch_size
        
        dw2 = np.dot(self.a1.T, dz2)
        db2 = np.sum(dz2, axis=0, keepdims=True)
        
        # Gradient of Hidden Layer
        da1 = np.dot(dz2, self.w2.T)
        dz1 = da1 * self.relu_derivative(self.z1)
        
        dw1 = np.dot(self.a0.T, dz1)
        db1 = np.sum(dz1, axis=0, keepdims=True)
        
        # 3. Update Weights (Gradient Descent)
        self.w2 -= self.lr * dw2
        self.b2 -= self.lr * db2
        self.w1 -= self.lr * dw1
        self.b1 -= self.lr * db1
        
        loss = -np.mean(np.log(probs[np.arange(batch_size), y_batch] + 1e-8))
        return loss

def load_and_preprocess_data(data_dir, img_size=64):
    if not os.path.exists(data_dir):
        raise FileNotFoundError(f"The directory {data_dir} does not exist.")

    images = []
    labels = []
    all_items = os.listdir(data_dir)
    class_names = [d for d in all_items if os.path.isdir(os.path.join(data_dir, d))]
    
    print(f"Loading data from: {data_dir}")
    for idx, label in enumerate(class_names):
        path = os.path.join(data_dir, label)
        files = [f for f in os.listdir(path) if f.lower().endswith(('.png', '.jpg', '.jpeg'))]
        for img_name in files[:100]: 
            img_path = os.path.join(path, img_name)
            img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)
            if img is not None:
                img = cv2.resize(img, (img_size, img_size))
                images.append(img)
                labels.append(idx)
                
    if len(images) == 0:
        raise ValueError("Zero images loaded. Check your dataset path.")

    X = np.array(images) / 255.0 
    y = np.array(labels)
    return X, y, class_names

def run_experiment():
    # --- ENSURE THIS PATH IS CORRECT ---
    data_path = 'C:/Users/hp user/Downloads/plantvillage-dataset/plantvillage dataset/color' 
    
    try:
        X, y, classes = load_and_preprocess_data(data_path, img_size=64)
        X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42)
        X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)
        
        X_train_flat = X_train.reshape(len(X_train), -1)
        X_test_flat = X_test.reshape(len(X_test), -1)

        print("\n--- Running Baselines ---")
        # Baseline 1: Logistic (Standard Random Init)
        W_log = np.random.randn(64*64, len(classes)) * 0.01
        y_pred_log = np.argmax(np.dot(X_test_flat, W_log), axis=1)
        
        # Baseline 2: KNN
        knn = KNeighborsClassifier(n_neighbors=3)
        knn.fit(X_train_flat, y_train)
        y_pred_knn = knn.predict(X_test_flat)
        
        print("\n--- Training Deep Scratch Model (100 Epochs) ---")
        # Hidden dimension of 128 provides enough complexity to learn leaf patterns
        model = DeepNeuralNetScratch(input_dim=64, hidden_dim=128, num_classes=len(classes), learning_rate=0.05)
        
        batch_size = 32
        for epoch in range(100):
            # Shuffle training data each epoch
            indices = np.random.permutation(len(X_train))
            X_shuffled = X_train[indices]
            y_shuffled = y_train[indices]
            
            epoch_loss = 0
            for i in range(0, len(X_train), batch_size):
                xb = X_shuffled[i:i+batch_size]
                yb = y_shuffled[i:i+batch_size]
                loss = model.train_step(xb, yb)
                epoch_loss += loss
            
            if epoch % 10 == 0:
                avg_loss = epoch_loss / (len(X_train) / batch_size)
                print(f"Epoch {epoch}, Avg Loss: {avg_loss:.4f}")

        # Final Evaluation
        test_probs = model.forward(X_test)
        final_preds = np.argmax(test_probs, axis=1)

        print("\n" + "="*30)
        print("      FINAL PROJECT RESULTS")
        print("="*30)
        print(f"Baseline 1 (Random/Log) Accuracy: {accuracy_score(y_test, y_pred_log):.4f}")
        print(f"Baseline 2 (KNN) Accuracy:        {accuracy_score(y_test, y_pred_knn):.4f}")
        print(f"Scratch Model Accuracy:           {accuracy_score(y_test, final_preds):.4f}")
        
        print("\nDetailed Report for Scratch Model:")
        print(classification_report(y_test, final_preds, target_names=classes))

    except Exception as e:
        print(f"\nFATAL ERROR: {e}")

if __name__ == "__main__":
    run_experiment()
