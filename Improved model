import numpy as np
import os
import cv2
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report
from sklearn.neighbors import KNeighborsClassifier

class DeepNeuralNetScratch:
    """
    Advanced Deep Neural Network implementation from scratch using NumPy.
    Upgrades:
    - 3 Layers (Input -> Hidden 1 -> Hidden 2 -> Output)
    - RGB Support (3 channels)
    - Learning Rate Decay (improves convergence)
    - Manual Backpropagation for multiple layers
    """
    def __init__(self, input_dim, hidden1, hidden2, num_classes, learning_rate=0.01):
        # Input dimension now accounts for 3 color channels (R, G, B)
        # 64 * 64 * 3 = 12288 features
        input_size = input_dim * input_dim * 3
        
        # Layer 1: Input to Hidden 1
        self.w1 = np.random.randn(input_size, hidden1) * np.sqrt(2. / input_size)
        self.b1 = np.zeros((1, hidden1))
        
        # Layer 2: Hidden 1 to Hidden 2
        self.w2 = np.random.randn(hidden1, hidden2) * np.sqrt(2. / hidden1)
        self.b2 = np.zeros((1, hidden2))
        
        # Layer 3: Hidden 2 to Output
        self.w3 = np.random.randn(hidden2, num_classes) * np.sqrt(2. / hidden2)
        self.b3 = np.zeros((1, num_classes))
        
        self.lr = learning_rate

    def relu(self, x):
        return np.maximum(0, x)

    def relu_derivative(self, x):
        return (x > 0).astype(float)

    def softmax(self, x):
        exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))
        return exp_x / np.sum(exp_x, axis=1, keepdims=True)

    def forward(self, X):
        # Flatten input (Batch, 64, 64, 3) -> (Batch, 12288)
        self.a0 = X.reshape(X.shape[0], -1)
        
        # Hidden Layer 1
        self.z1 = np.dot(self.a0, self.w1) + self.b1
        self.a1 = self.relu(self.z1)
        
        # Hidden Layer 2
        self.z2 = np.dot(self.a1, self.w2) + self.b2
        self.a2 = self.relu(self.z2)
        
        # Output Layer
        self.z3 = np.dot(self.a2, self.w3) + self.b3
        self.probs = self.softmax(self.z3)
        return self.probs

    def train_step(self, X_batch, y_batch):
        batch_size = X_batch.shape[0]
        
        # 1. Forward Pass
        probs = self.forward(X_batch)
        
        # 2. Backward Pass
        y_one_hot = np.zeros_like(probs)
        y_one_hot[np.arange(batch_size), y_batch] = 1
        
        # Output layer gradients
        dz3 = (probs - y_one_hot) / batch_size
        dw3 = np.dot(self.a2.T, dz3)
        db3 = np.sum(dz3, axis=0, keepdims=True)
        
        # Hidden layer 2 gradients
        da2 = np.dot(dz3, self.w3.T)
        dz2 = da2 * self.relu_derivative(self.z2)
        dw2 = np.dot(self.a1.T, dz2)
        db2 = np.sum(dz2, axis=0, keepdims=True)
        
        # Hidden layer 1 gradients
        da1 = np.dot(dz2, self.w2.T)
        dz1 = da1 * self.relu_derivative(self.z1)
        dw1 = np.dot(self.a0.T, dz1)
        db1 = np.sum(dz1, axis=0, keepdims=True)
        
        # 3. Update Weights
        self.w3 -= self.lr * dw3
        self.b3 -= self.lr * db3
        self.w2 -= self.lr * dw2
        self.b2 -= self.lr * db2
        self.w1 -= self.lr * dw1
        self.b1 -= self.lr * db1
        
        loss = -np.mean(np.log(probs[np.arange(batch_size), y_batch] + 1e-8))
        return loss

def load_and_preprocess_data(data_dir, img_size=64):
    if not os.path.exists(data_dir):
        raise FileNotFoundError(f"The directory {data_dir} does not exist.")

    images = []
    labels = []
    all_items = os.listdir(data_dir)
    class_names = [d for d in all_items if os.path.isdir(os.path.join(data_dir, d))]
    
    print(f"Loading RGB data from: {data_dir}")
    for idx, label in enumerate(class_names):
        path = os.path.join(data_dir, label)
        files = [f for f in os.listdir(path) if f.lower().endswith(('.png', '.jpg', '.jpeg'))]
        # Processing 100 images per class for efficiency
        for img_name in files[:100]: 
            img_path = os.path.join(path, img_name)
            # CHANGED: Loading in Color (RGB) instead of Grayscale
            img = cv2.imread(img_path)
            if img is not None:
                img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
                img = cv2.resize(img, (img_size, img_size))
                images.append(img)
                labels.append(idx)
                
    if len(images) == 0:
        raise ValueError("Zero images loaded.")

    X = np.array(images) / 255.0 
    y = np.array(labels)
    return X, y, class_names

def run_experiment():
    data_path = 'C:/Users/hp user/Downloads/plantvillage-dataset/plantvillage dataset/color' 
    
    try:
        X, y, classes = load_and_preprocess_data(data_path, img_size=64)
        X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42)
        X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)
        
        X_train_flat = X_train.reshape(len(X_train), -1)
        X_test_flat = X_test.reshape(len(X_test), -1)

        print("\n--- Running Baselines ---")
        W_log = np.random.randn(X_train_flat.shape[1], len(classes)) * 0.01
        y_pred_log = np.argmax(np.dot(X_test_flat, W_log), axis=1)
        
        knn = KNeighborsClassifier(n_neighbors=3)
        knn.fit(X_train_flat, y_train)
        y_pred_knn = knn.predict(X_test_flat)
        
        print("\n--- Training Deep RGB Model (100 Epochs) ---")
        # Added a second hidden layer (256, 128)
        model = DeepNeuralNetScratch(input_dim=64, hidden1=256, hidden2=128, num_classes=len(classes), learning_rate=0.05)
        
        batch_size = 32
        for epoch in range(100):
            indices = np.random.permutation(len(X_train))
            X_shuffled = X_train[indices]
            y_shuffled = y_train[indices]
            
            epoch_loss = 0
            for i in range(0, len(X_train), batch_size):
                xb = X_shuffled[i:i+batch_size]
                yb = y_shuffled[i:i+batch_size]
                loss = model.train_step(xb, yb)
                epoch_loss += loss
            
            # Implementation of Learning Rate Decay
            if epoch % 30 == 0 and epoch > 0:
                model.lr *= 0.5
                print(f"Decaying learning rate to: {model.lr}")

            if epoch % 10 == 0:
                avg_loss = epoch_loss / (len(X_train) / batch_size)
                print(f"Epoch {epoch}, Avg Loss: {avg_loss:.4f}")

        test_probs = model.forward(X_test)
        final_preds = np.argmax(test_probs, axis=1)

        print("\n" + "="*30)
        print("      FINAL PROJECT RESULTS (RGB)")
        print("="*30)
        print(f"Baseline 1 (Random/Log) Accuracy: {accuracy_score(y_test, y_pred_log):.4f}")
        print(f"Baseline 2 (KNN) Accuracy:        {accuracy_score(y_test, y_pred_knn):.4f}")
        print(f"Scratch Model Accuracy:           {accuracy_score(y_test, final_preds):.4f}")
        
        print("\nDetailed Report for RGB Model:")
        print(classification_report(y_test, final_preds, target_names=classes))

    except Exception as e:
        print(f"\nFATAL ERROR: {e}")

if __name__ == "__main__":
    run_experiment()
