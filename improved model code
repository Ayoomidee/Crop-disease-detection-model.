import numpy as np
import os
import cv2
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report
from sklearn.neighbors import KNeighborsClassifier

class CNNScratch:
    """
    Advanced CNN implementation from scratch using NumPy.
    Features:
    - Conv2D Layer with FULL Backpropagation
    - Max Pooling (Downsampling)
    - Learning Rate Decay
    - Optimized Convolution logic
    """
    def __init__(self, num_classes, learning_rate=0.01):
        # Layer 1: Conv2D (8 filters of 3x3)
        self.num_filters = 8
        self.f_size = 3
        # Initialize filters with Xavier/Glorot initialization
        self.filters = np.random.randn(8, 3, 3, 3) * np.sqrt(2. / (3 * 3 * 3))
        
        # Max Pooling (2x2) on 64x64 input with 3x3 conv (no padding) 
        # Output after Conv: 62x62 -> Output after Pool: 31x31
        self.flattened_size = 31 * 31 * 8
        
        # MLP Head
        self.w1 = np.random.randn(self.flattened_size, 128) * np.sqrt(2. / self.flattened_size)
        self.b1 = np.zeros((1, 128))
        self.w2 = np.random.randn(128, num_classes) * np.sqrt(2. / 128)
        self.b2 = np.zeros((1, num_classes))
        
        self.lr = learning_rate

    def relu(self, x):
        return np.maximum(0, x)

    def relu_derivative(self, x):
        return (x > 0).astype(float)

    def softmax(self, x):
        exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))
        return exp_x / np.sum(exp_x, axis=1, keepdims=True)

    def conv_forward(self, X):
        self.X_conv = X # Store for backprop
        b, h, w, c = X.shape
        out_h, out_w = h - self.f_size + 1, w - self.f_size + 1
        out = np.zeros((b, out_h, out_w, self.num_filters))
        
        # Optimized sliding window using loops (NumPy vectorized internally where possible)
        for i in range(out_h):
            for j in range(out_w):
                region = X[:, i:i+self.f_size, j:j+self.f_size, :]
                for f in range(self.num_filters):
                    out[:, i, j, f] = np.sum(region * self.filters[f], axis=(1, 2, 3))
        return out

    def maxpool(self, X):
        b, h, w, c = X.shape
        out = np.zeros((b, h//2, w//2, c))
        for i in range(0, h-1, 2):
            for j in range(0, w-1, 2):
                out[:, i//2, j//2, :] = np.max(X[:, i:i+2, j:j+2, :], axis=(1, 2))
        return out

    def forward(self, X):
        # 1. Conv + ReLU
        self.z_conv = self.conv_forward(X)
        self.a_conv = self.relu(self.z_conv)
        
        # 2. Max Pool
        self.pool_out = self.maxpool(self.a_conv)
        
        # 3. Flatten
        self.a0 = self.pool_out.reshape(X.shape[0], -1)
        
        # 4. Hidden Layer
        self.z1 = np.dot(self.a0, self.w1) + self.b1
        self.a1 = self.relu(self.z1)
        
        # 5. Output Layer
        self.z2 = np.dot(self.a1, self.w2) + self.b2
        self.probs = self.softmax(self.z2)
        return self.probs

    def train_step(self, X_batch, y_batch):
        batch_size = X_batch.shape[0]
        
        # Forward
        probs = self.forward(X_batch)
        
        # --- Backward Pass (Backpropagation) ---
        y_one_hot = np.zeros_like(probs)
        y_one_hot[np.arange(batch_size), y_batch] = 1
        
        # Output Layer Gradients
        dz2 = (probs - y_one_hot) / batch_size
        dw2 = np.dot(self.a1.T, dz2)
        db2 = np.sum(dz2, axis=0, keepdims=True)
        
        # Hidden Layer Gradients
        da1 = np.dot(dz2, self.w2.T)
        dz1 = da1 * self.relu_derivative(self.z1)
        dw1 = np.dot(self.a0.T, dz1)
        db1 = np.sum(dz1, axis=0, keepdims=True)
        
        # --- Conv Layer Backprop (The "Missing" Step) ---
        # 1. Backprop through Flatten/Pooling
        da0 = np.dot(dz1, self.w1.T)
        d_pool = da0.reshape(self.pool_out.shape)
        
        # 2. Simple Upsample for MaxPool (simplified backprop)
        d_conv = np.repeat(np.repeat(d_pool, 2, axis=1), 2, axis=2)
        # Match dimensions if conv_out was odd
        d_conv = d_conv[:, :self.a_conv.shape[1], :self.a_conv.shape[2], :]
        d_z_conv = d_conv * self.relu_derivative(self.z_conv)
        
        # 3. Gradients for Filters
        d_filters = np.zeros_like(self.filters)
        for i in range(self.f_size):
            for j in range(self.f_size):
                region = self.X_conv[:, i:i+d_z_conv.shape[1], j:j+d_z_conv.shape[2], :]
                for f in range(self.num_filters):
                    d_filters[f] += np.sum(region * d_z_conv[:, :, :, f:f+1], axis=(0, 1, 2))
        
        # Update All Weights
        self.w2 -= self.lr * dw2
        self.b2 -= self.lr * db2
        self.w1 -= self.lr * dw1
        self.b1 -= self.lr * db1
        self.filters -= self.lr * d_filters
        
        loss = -np.mean(np.log(probs[np.arange(batch_size), y_batch] + 1e-8))
        return loss

def load_and_preprocess_data(data_dir, img_size=64):
    if not os.path.exists(data_dir):
        raise FileNotFoundError(f"The directory {data_dir} does not exist.")

    images, labels = [], []
    class_names = sorted([d for d in os.listdir(data_dir) if os.path.isdir(os.path.join(data_dir, d))])
    
    print(f"Loading data from: {data_dir}")
    for idx, label in enumerate(class_names):
        path = os.path.join(data_dir, label)
        files = [f for f in os.listdir(path) if f.lower().endswith(('.png', '.jpg', '.jpeg'))]
        # Using 80 images per class for a better balance of speed and learning
        for img_name in files[:80]: 
            img_path = os.path.join(path, img_name)
            img = cv2.imread(img_path)
            if img is not None:
                img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
                img = cv2.resize(img, (img_size, img_size))
                images.append(img)
                labels.append(idx)
                
    X = np.array(images) / 255.0 
    y = np.array(labels)
    return X, y, class_names

def run_experiment():
    data_path = 'C:/Users/hp user/Downloads/plantvillage-dataset/plantvillage dataset/color' 
    
    try:
        X, y, classes = load_and_preprocess_data(data_path, img_size=64)
        X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.2, random_state=42)
        X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)
        
        print(f"\n--- Training CNN with Conv-Backprop (50 Epochs) ---")
        model = CNNScratch(num_classes=len(classes), learning_rate=0.01)
        
        batch_size = 32
        for epoch in range(50):
            indices = np.random.permutation(len(X_train))
            X_shuffled, y_shuffled = X_train[indices], y_train[indices]
            
            epoch_loss = 0
            for i in range(0, len(X_train), batch_size):
                xb, yb = X_shuffled[i:i+batch_size], y_shuffled[i:i+batch_size]
                loss = model.train_step(xb, yb)
                epoch_loss += loss
            
            if epoch % 10 == 0:
                print(f"Epoch {epoch}, Loss: {epoch_loss / (len(X_train)/batch_size):.4f}")
            if epoch == 25: model.lr *= 0.5 # Simple decay

        final_preds = np.argmax(model.forward(X_test), axis=1)
        print(f"\nCNN Scratch Accuracy: {accuracy_score(y_test, final_preds):.4f}")
        print("\nDetailed Report:")
        print(classification_report(y_test, final_preds, target_names=classes))

    except Exception as e:
        print(f"\nERROR: {e}")

if __name__ == "__main__":
    run_experiment()
